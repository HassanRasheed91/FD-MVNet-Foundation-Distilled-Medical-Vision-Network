# ğŸ¥ Foundation Model-Enhanced COVID Detection System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ **97.59% COVID Detection Accuracy with 297x Parameter Reduction**

A cutting-edge **foundation model ensemble** with **knowledge distillation** framework for COVID-19 detection from chest CT scans. This system combines **BiomedCLIP**, **DINOv2**, and **OpenAI CLIP** foundation models to create lightweight, deployable models suitable for resource-constrained clinical environments.

---

## ğŸš€ **Quick Start**

### **Option 1: Kaggle (Recommended)**
```bash
# 1. Upload this repository as a Kaggle dataset
# 2. Create new notebook with GPU enabled
# 3. Run these commands:

!pip install -r /kaggle/input/covid-foundation-models/requirements.txt
!cp /kaggle/input/covid-foundation-models/*.py ./
!python main.py
```

### **Option 2: Local Setup**
```bash
git clone https://github.com/HassanRasheed91/FD-MVNet-Foundation-Distilled-Medical-Vision-Network.git
cd FD-MVNet-Foundation-Distilled-Medical-Vision-Network
pip install -r requirements.txt
python main.py
```

---

## ğŸ† **Key Achievements**

| Metric | Teacher Ensemble | Lightweight Student | Improvement |
|--------|------------------|---------------------|-------------|
| **Accuracy** | 98.66% | **97.59%** | -1.07% |
| **Parameters** | 435.0M | **1.5M** | **297x reduction** |
| **Model Size** | 1740 MB | **6.0 MB** | **290x smaller** |
| **AUC-ROC** | 0.9985 | **0.9958** | Maintained |
| **Inference Speed** | 45ms | **8ms** | **5.6x faster** |

### ğŸ¯ **Clinical Performance**
- **Sensitivity**: 96.8% (COVID detection)
- **Specificity**: 97.6% (Non-COVID detection)  
- **Deployment Ready**: Edge device compatible
- **Real-time**: <10ms inference on CPU

---

## ğŸ”¬ **Technical Innovation**

### **Foundation Model Ensemble Teacher**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§  Foundation Model Teacher Ensemble (435.0M params)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”¬ BiomedCLIP     â”‚  Medical-specific representations  â”‚
â”‚  ğŸ¯ DINOv2         â”‚  Self-supervised visual features   â”‚  
â”‚  ğŸ‘ï¸ OpenAI CLIP    â”‚  General vision understanding      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    ğŸ“š Knowledge Distillation
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ğŸ’¡ Lightweight Student (1.5M params)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ—ï¸ Medical-Optimized Architecture                     â”‚  
â”‚  âš¡ Efficient Blocks with SE Attention                 â”‚
â”‚  ğŸ¯ Depthwise Separable Convolutions                   â”‚
â”‚  ğŸ“± Mobile-Friendly Design                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Advanced Knowledge Distillation**
- **Multi-Teacher Ensemble**: Combines knowledge from multiple foundation models
- **Temperature Scaling**: Soft target generation with T=3.0
- **Ensemble Consistency**: Ensures student aligns with all teachers
- **Feature-Level Transfer**: Deep feature alignment beyond just predictions

---

## ğŸ“Š **Research Results**

### **Published Performance** (From Paper: *Computers in Biology and Medicine*, 2021)
Based on the **Sugeno fuzzy integral ensemble** with four pre-trained models:
- **Paper Accuracy**: 98.93%
- **Paper Sensitivity**: 98.93%
- **Models Used**: VGG-11, GoogLeNet, SqueezeNet v1.1, Wide ResNet-50-2
- **Dataset**: SARS-COV-2 CT-scan dataset (2,481 images)

### **Current Implementation Results**
Our **foundation model enhanced** version achieves:
- **Student Accuracy**: 97.59%
- **Teacher Accuracy**: 98.66%
- **Parameter Reduction**: 297x
- **Model Compression**: 290x size reduction
- **Maintained AUC**: 0.9958 vs 0.9985

### **Key Improvements Over Paper**
1. **Foundation Model Integration**: Added BiomedCLIP, DINOv2, OpenAI CLIP
2. **Extreme Compression**: 297x parameter reduction vs traditional ensemble
3. **Real-time Deployment**: 6MB model size for edge devices
4. **Medical Optimization**: Enhanced augmentations and architecture

---

## ğŸ—ï¸ **Architecture Overview**

### **File Structure**
```
foundation-covid-detection/
â”œâ”€â”€ ğŸ“„ main.py              # Main execution pipeline
â”œâ”€â”€ ğŸ§  models.py            # Foundation models & architectures  
â”œâ”€â”€ ğŸ“ training.py          # Knowledge distillation framework
â”œâ”€â”€ ğŸ“Š visualization.py     # Research visualization engine
â”œâ”€â”€ ğŸ“ dataset.py           # Enhanced data pipeline
â”œâ”€â”€ ğŸ“‹ requirements.txt     # Dependencies
â”œâ”€â”€ ğŸ“– README.md           # This file
â””â”€â”€ ğŸ“ results/            # Generated outputs
    â”œâ”€â”€ training_curves.png
    â”œâ”€â”€ model_comparison.png
    â”œâ”€â”€ confusion_matrices.png
    â”œâ”€â”€ roc_curves.png
    â”œâ”€â”€ performance_summary.csv
    â””â”€â”€ foundation_models.pth
```

## ğŸ› ï¸ **Installation & Setup**

### **System Requirements**
- **Python**: 3.8+
- **GPU**: NVIDIA GPU with 8GB+ VRAM (recommended)
- **CPU**: 8+ cores for CPU-only training
- **RAM**: 16GB+ recommended
- **Storage**: 10GB for models and datasets

### **Dependencies**
```bash
# Core ML Framework
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Foundation Models
open_clip_torch==2.23.0
transformers==4.35.2
timm>=0.9.0

# Computer Vision & Data Processing
albumentations>=1.3.0
opencv-python>=4.8.0
Pillow>=9.5.0

# Scientific Computing
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
scipy>=1.10.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0

# Utilities
tqdm>=4.65.0
tensorboard>=2.13.0
```

### **Quick Installation**
```bash
# Method 1: Using pip
pip install -r requirements.txt

# Method 2: Using conda
conda env create -f environment.yml
conda activate covid-detection
```

---

## ğŸ“Š **Dataset Setup**

### **Expected Directory Structure**
```
dataset_split/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ COVID/          # COVID-positive CT scans
â”‚   â”‚   â”œâ”€â”€ img001.png
â”‚   â”‚   â”œâ”€â”€ img002.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ NonCOVID/       # COVID-negative CT scans
â”‚       â”œâ”€â”€ img001.png
â”‚       â”œâ”€â”€ img002.png
â”‚       â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ COVID/
â”‚   â””â”€â”€ NonCOVID/
â””â”€â”€ test/
    â”œâ”€â”€ COVID/
    â””â”€â”€ NonCOVID/
```

### **Supported Datasets**
- **SARS-COV-2 CT-scan Dataset** ([Kaggle](https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset))
- **Custom datasets** following the above structure

---

## ğŸš€ **Usage**

### **Basic Usage**
```python
# Run complete experiment
python main.py

# Expected output:
# ğŸ“ FOUNDATION MODEL COVID DETECTION EXPERIMENT
# ================================================================================
# 1ï¸âƒ£ Setting up enhanced data pipeline...
# 2ï¸âƒ£ Initializing foundation models...
# 3ï¸âƒ£ Stage 1: Training Foundation Teacher (25 epochs)...
# 4ï¸âƒ£ Stage 2: Training Student with Distillation (40 epochs)...
# 5ï¸âƒ£ Comprehensive Evaluation & Visualization...
# ğŸ† SUCCESS! ACHIEVED 97%+ ACCURACY!
```

### **Custom Configuration**
```python
from main import run_foundation_model_experiment

# Custom training configuration
results = run_foundation_model_experiment(
    data_dir='/path/to/dataset',
    device=torch.device('cuda'),
    teacher_epochs=50,      # More epochs for better teacher
    student_epochs=100      # More epochs for better student
)

print(f"Final Accuracy: {results['student_accuracy']:.2f}%")
print(f"Parameter Reduction: {results['reduction_ratio']:.1f}x")
```

### **Individual Components**
```python
# Use individual components
from models import FoundationModelTeacher, LightweightMedicalStudent
from training import FoundationModelTrainer

# Initialize models
teacher = FoundationModelTeacher(num_classes=2)
student = LightweightMedicalStudent(num_classes=2, width_multiplier=1.2)

# Train with knowledge distillation
trainer = FoundationModelTrainer(device='cuda')
trainer.train_teacher(teacher, train_loader, val_loader, epochs=25)
trainer.train_student(student, teacher, train_loader, val_loader, epochs=40)
```

---

## ğŸ“ˆ **Training Details**

### **Training Strategy**
1. **Stage 1**: Foundation Model Teacher Training
   - Multi-rate learning: 1e-5 for foundation models, 1e-4 for classical
   - Cosine annealing schedule
   - Early stopping with patience=10
   - Gradient clipping (max_norm=1.0)

2. **Stage 2**: Knowledge Distillation  
   - OneCycleLR scheduler (max_lr=1e-3)
   - Advanced distillation loss (Î±=0.8, T=3.0)
   - Extended patience=30 for student training
   - Feature-level alignment

### **Loss Functions**
```python
# Multi-component distillation loss
L_total = (1-Î±) Ã— L_hard + Î± Ã— L_soft + Î» Ã— L_ensemble

Where:
- L_hard: Cross-entropy with ground truth
- L_soft: KL divergence with teacher predictions  
- L_ensemble: Consistency across all teacher models
- Î± = 0.8, Î» = 0.1, Temperature = 3.0
```

### **Data Augmentation**
```python
# Medical-optimized augmentations
- Geometric: HorizontalFlip, Rotate(Â±10Â°), ShiftScaleRotate
- Intensity: RandomBrightnessContrast, CLAHE, HSV
- Robustness: GaussNoise, GaussianBlur, Compression
- Regularization: CoarseDropout
- Normalization: ImageNet statistics for foundation models
```

---

## ğŸ“Š **Evaluation Metrics**

### **Classification Metrics**
- **Accuracy**: Overall classification accuracy
- **Precision**: Class-wise and weighted precision
- **Recall/Sensitivity**: COVID detection rate
- **Specificity**: Non-COVID detection rate  
- **F1-Score**: Harmonic mean of precision/recall
- **AUC-ROC**: Area under ROC curve
- **AUC-PR**: Area under Precision-Recall curve

### **Efficiency Metrics**
- **Parameter Count**: Total trainable parameters
- **Model Size**: Storage requirements (MB)
- **Inference Time**: Forward pass latency
- **FLOPS**: Floating point operations
- **Memory Usage**: Peak GPU/CPU memory

---

## ğŸ“ **Output Files**

### **Generated Results**
```
results/
â”œâ”€â”€ ğŸ“Š Visualizations
â”‚   â”œâ”€â”€ training_curves.png              # Training progress
â”‚   â”œâ”€â”€ model_comparison.png             # Performance comparison  
â”‚   â”œâ”€â”€ confusion_matrices.png           # Classification results
â”‚   â”œâ”€â”€ roc_curves.png                   # ROC analysis
â”‚   â”œâ”€â”€ precision_recall_curves.png      # PR analysis
â”‚   â”œâ”€â”€ comprehensive_metrics.png        # Metrics table
â”‚   â””â”€â”€ knowledge_distillation_analysis.png
â”‚
â”œâ”€â”€ ğŸ“ˆ Performance Data
â”‚   â”œâ”€â”€ performance_summary.csv          # Model comparison
â”‚   â”œâ”€â”€ comprehensive_metrics.csv        # Detailed metrics
â”‚   â””â”€â”€ research_highlights.json         # Key achievements
â”‚
â”œâ”€â”€ ğŸ§  Trained Models
â”‚   â”œâ”€â”€ foundation_models.pth            # Teacher & student weights
â”‚   â”œâ”€â”€ teacher_best.pth                 # Best teacher checkpoint
â”‚   â””â”€â”€ student_best.pth                 # Best student checkpoint
â”‚
â”œâ”€â”€ ğŸ“ Documentation
â”‚   â”œâ”€â”€ research_summary.md              # Publication summary
â”‚   â”œâ”€â”€ training.log                     # Detailed training logs
â”‚   â””â”€â”€ foundation_experiment_results.pth # Complete results
```

---

## ğŸ”¬ **Research Applications**

### **Clinical Deployment**
```python
# Load trained student model for inference
import torch
from models import LightweightMedicalStudent

# Load lightweight model (6.0 MB)
model = LightweightMedicalStudent(num_classes=2)
checkpoint = torch.load('results/student_best.pth')
model.load_state_dict(checkpoint)
model.eval()

# Real-time inference
def predict_covid(ct_scan_image):
    with torch.no_grad():
        prediction = model(ct_scan_image)
        probability = torch.softmax(prediction, dim=1)
        return probability[0][1].item()  # COVID probability

# Edge deployment ready
covid_prob = predict_covid(patient_scan)
print(f"COVID-19 Probability: {covid_prob:.3f}")
```

### **Research Extensions**
- **Multi-class Classification**: Extend to Normal/Pneumonia/COVID
- **Segmentation**: Add lesion segmentation capabilities  
- **Cross-modal**: Integrate with clinical text data
- **Federated Learning**: Distributed training across hospitals
- **Explainability**: Add Grad-CAM visualization
- **Uncertainty Quantification**: Bayesian neural networks

---

### **Related Work**
```bibtex
@article{biomedclip2024,
  title={BiomedCLIP: Large-scale biomedical vision-language pre-training},
  author={Zhang, Sheng and Xu, Yanbo and Usuyama, Naoto and others},
  journal={arXiv preprint arXiv:2303.00915},
  year={2024}
}

@article{dinov2023,
  title={DINOv2: Learning Robust Visual Representations without Supervision},
  author={Oquab, Maxime and Darcet, TimothÃ©e and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}
```

---

## ğŸ¤ **Contributing**

### **Development Setup**
```bash
# Clone repository
git clone https://github.com/HassanRasheed91/FD-MVNet-Foundation-Distilled-Medical-Vision-Network.git
cd FD-MVNet-Foundation-Distilled-Medical-Vision-Network

# Create development environment
conda create -n covid-dev python=3.9
conda activate covid-dev
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install
```

### **Contribution Guidelines**
1. **Fork** the repository
2. **Create** feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** changes (`git commit -m 'Add amazing feature'`)
4. **Push** to branch (`git push origin feature/amazing-feature`)
5. **Open** Pull Request

---

## ğŸ†˜ **Support & FAQ**

### **Common Issues**

**Q: Out of Memory Error**
```python
# Reduce batch size in main.py
batch_size = 8  # instead of 16

# Enable gradient checkpointing
model.gradient_checkpointing_enable()

# Use mixed precision training
from torch.cuda.amp import autocast, GradScaler
```

**Q: Foundation Models Not Loading**
```python
# Check internet connection for model downloads
# Verify transformers and open_clip versions
pip install --upgrade transformers open_clip_torch

# Use fallback classical models
# Set use_foundation_models=False in config
```

**Q: Dataset Path Issues**
```python
# Update dataset path in main.py
DATASET_PATH = '/your/actual/dataset/path'

# Verify directory structure
python -c "from dataset import ResearchGradeCOVIDDataset; ResearchGradeCOVIDDataset('path', 'train')"
```

### **Performance Optimization**
- **GPU**: Use Tesla V100 for fastest training
- **CPU**: 16+ cores recommended for CPU-only training  
- **Mixed Precision**: Reduces memory usage by 50%
- **DataLoader**: Increase num_workers for faster data loading

### **Getting Help**
- ğŸ“§ **Email**: 221980038@gift.edu.pk
- ğŸ’¬ **Issues**: [GitHub Issues](https://github.com/HassanRasheed91/FD-MVNet-Foundation-Distilled-Medical-Vision-Network/issues)

---

## ğŸ“œ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Hassan Rasheed

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

---

## ğŸŒŸ **Acknowledgments**

- **Foundation Models**: BiomedCLIP, DINOv2, OpenAI CLIP teams
- **Datasets**: SARS-COV-2 CT-scan dataset contributors
- **Computing**: Thanks to Kaggle for GPU resources
- **Community**: PyTorch and Hugging Face ecosystems
- **Inspiration**: Knowledge distillation and medical AI research

---

## ğŸ“Š **Project Stats**

**ğŸ“ˆ Impact Metrics:**
- **97.59% Accuracy** - High-performance COVID detection
- **297x Compression** - Massive parameter reduction
- **<10ms Inference** - Real-time clinical deployment
- **6.0 MB Model** - Edge device compatible

---

<div align="center">

### ğŸ¯ **Ready to achieve 97%+ COVID detection accuracy?**

**[â­ Star this repo](https://github.com/HassanRasheed91/FD-MVNet-Foundation-Distilled-Medical-Vision-Network)** â€¢ **[ğŸ´ Fork it](https://github.com/HassanRasheed91/FD-MVNet-Foundation-Distilled-Medical-Vision-Network/fork)** 

**Built with â¤ï¸ for advancing medical AI research**

</div>
